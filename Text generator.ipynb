{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc098b3",
   "metadata": {},
   "source": [
    "# Text generator using BART modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca152a",
   "metadata": {},
   "source": [
    "# Install these modules first\n",
    "Transformers, pyTorch\n",
    "for better output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cff964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the BART model and tokenizer\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "766f49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the pretrained weights for BART\n",
    "# here, we use facebook's bart-large model\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0) # takes a while to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7889a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the raw text tokenizer for the BART model\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ffeb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <mask> tag is a special token\n",
    "# that is used to specify a missing token for the BART model\n",
    "sent = \"We wrote a <mask> to the BART model. So we can use it to <mask> text. So that we can <mask> the text easily.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4a37ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing(tokenizing) the text as input for the BART model\n",
    "tokenized_sent = tokenizer(sent, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fde62bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated encoded ids\n",
    "generated_encoded = bart_model.generate(tokenized_sent['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8e14add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We wrote a model that is similar to the BART model. So we can use it to represent the text. So that we can read the text easily.\n"
     ]
    }
   ],
   "source": [
    "# decoding the generated encoded ids\n",
    "def fill_masks(text, tokenizer, model, max_masks=5): # Maximum number of masks to fill and we can adjust it\n",
    "    max_length = 1024  # Maximum length of the generated text\n",
    "    for _ in range(max_masks):\n",
    "        if \"<mask>\" not in text:\n",
    "            break\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        output_ids = model.generate(input_ids, max_length=max_length)\n",
    "        decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        # Replace only the first <mask> in the text with the new prediction\n",
    "        text = decoded\n",
    "    return text\n",
    "\n",
    "filled_full_text = fill_masks(sent, tokenizer, bart_model)\n",
    "print(filled_full_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
